\documentclass[12pt]{article}

\usepackage{notestyle}

\graphicspath{{./img/}}


\title{Appunti Database}
\author{Brendon Mendicino}



\begin{document}

\maketitle
\newpage
\tableofcontents
\newpage


\section{Introduzione}\label{sec:introduzione}


KDD: Knowledge Discovery from Data

\paragraph{Tecniche di data mining}
\begin{itemize}
    \item Regole di associazione: usate per trovare delle relazioni frequenti all'interno del database. Ad esempio: chi compra pannolini compra anche birra, il 2$\%$ degli scontrini contegono entrambe gli oggetti, il 30$\%$ degli scontrini che contengono pannalini contengono anche birra. Grazie alle regole di associazione si possono fare dei tipi di analisi come la basket analisys, ma puo essere utile anche per le raccomanadazioni.
    \item Classificazione: i classificatori predicono etichette discrete, esempio: nella posta elettronica alcune mail vengono segnate come spam. La classificazione definisce un modello per definire le predizioni, a volte non \`e sempre possibile creare dei modelli interpretabili ovvero dare una ragione per una determinata scelta.
    \item Clustering: gli algoritmi creano dei gruppi che raggruppano gli oggetti in esame, senza per\`o dare delle motivazioni delle scelte effettuate.
\end{itemize}





\section{Data Warehouse}
Un DW  \`e una base dati di supporto alla decisioni, che \`e mantenuta separatamente dalla base di dati operativa dell'azienda. I dati al suo interno sono:
\begin{itemize}
    \item orientati ai soggetti di interesse;
    \item integrati e consistenti;
    \item dipendenti dal tempo;
    \item non volatili;
    \item utilizzati per il supporto alle decisioni aziendali;
\end{itemize}


Per la progettazione concettuale di un DW, non esiste un formalismo universale, il modello ER non \`e adatto ma viene invece utilizzata il modello \textbf{Dimensional Fact Model}.

Il DFM \`e composto da:
\begin{itemize}
    \item Fatto: modella un insieme di eventi di interesse, che evolvono nel tempo (che pu\`o overe diversa granuralit\`a).
    \item Dimensioni: sono gli attribuiti di un fatto, generalmento sono categorici.
    \item Misure: discrive una caratteristica numerica di un fatto.
\end{itemize}
Sulle dimensioni si possono definire delle gerarachie, che definiscono di fatto una dipendenza funzionale tra gli attributi, quindi di 1 a n. Ad esempio: \textbf{data} ha un arco \textbf{mese}, una data ha uno ed un solo mese (1 a n).

I costrutti avanazati sono:
\begin{itemize}
    \item archi opzionali;
    \item dimensioni opzionali;
    \item attributo descrittivo: sono delle informazioni utili all'utente ma su cui non verteranno le interragazioni (ad esempio non si far\`a mai la group by su un indirizzo);
    \item non-additivit\`a: non si pu\`o fare la somma sulla metrica, il motivo \`e che non \`e modellato in modo tale da fare la somma;
\end{itemize}




\begin{itemize}
    \item Fatto: fenomeno di studio;
    \item Misure: attributi del fatto;
    \item Dimensioni: tabelle collegate al fatto;
\end{itemize}


\textbf{Schema a stella}:


\textbf{Snoflawke scheme}: si esplicitano le dipendenze funzionali, questo per\`o comporta un aumento delle operazioni di join.

Nella realt\`a lo snowflake \`e raramente utilizzato, il motivo \`e che il costo delle join pu\`o diventare oneroso. Un caso di utilizzo dello snowflake \`e quando si hanno dei dati condivisi.

\textbf{Archi multipli}:


\textbf{Dimensioni degeneri}: sono delle dimensioni con un solo attributo, questo si perch\`e nello stato attuale non si hanno delle specifiche per quell'attributo ma nel futuro si potrebbe facilmente estendere. Un'altra soluzione potrebbe essere un push down delle dimensioni degeri nella tabella dei fatti.

\textbf{Junk Dimension}: si pu\`o creare una dimensione che contenga tutte le dimensioni degeneri, le informazioni sono collegate semanticamente, \`e anche possibile unire delle informazioni scorrelate ma non \`e una scelta poco corretta, una soluzione potrebbe essere avere pi\`u junk dimensions.


\section{Analisi}
Sfruttando solo l'SQL \`e molto difficile fare delle analisi su un dw, infatti volendo calcolare delle operazioni per due argomenti diversi si devono fare pi\`u query. Estendendo il SQL si pu\`o, ad esempio, effettuare pi\`u operazioni leggendo una sola volta la tabella, ed effettuando il minor numero di join possibile.

\paragraph{Analisi OLAP}
I tipi di operazione sono:
\begin{itemize}
    \item roll up: riducendo il livello di dettaglio del dato, ovvero eliminare una o pi\`u clausole della groupby o navigare la gerarchia verso l'esterno;
    \item drill down: si aumenta il livello di dettaglio oppure si aggiunge una dimensione di analisi;
    \item slice and dice: consentono di ridurre il volume dei dati selezionando un sottogruppo dei dati di partenza;
    \item tabelle pivot: come viene mostrato il dato;
    \item ordinamento: ordinamento in base agli attributi;
\end{itemize}
Queste operazioni possono essere fatte con pi\`u o una query.

\subsection{Finestra di calcolo}
Una finestra di calcolo fa dei calcoli a partire da una query sottostante, la finestra ha 3 operazioni sottostanti:
\begin{itemize}
    \item partizionamento (\textbf{partition by}): partizionamnto dei dati, divide i record in gruppi a partire dall'attributo selezionato;
    \item ordinamento (\textbf{order by}): si definisce il criterio di ordinamento delle righe all'interno dei partizionamenti;
    \item finestra di aggregazione (\textbf{over}): porzione di dati, specifica per ogni riga di dato, su cui effettuare dei calcoli;
\end{itemize}
\begin{example}{}{}
    Data la tabella Vendite(\underline{Citt\`a}, \underline{Mese}, Importo), calcolare per ogni citt\`a la media delle vendite per il mese corrente ed i due precedenti.

\begin{lstlisting}[language=sql]
SELECT Citta, Mese, Importo,
    AVG(Importo) OVER (PARTITION BY Citta)
                        ORDER BY Mese
                        ROWS 2 PRECEDING)
    AS MediaMobile
FROM Vendite;
\end{lstlisting}
\end{example}
Quando la finestra \`e incompleta il calcolo \`e effettuato sulla parte presente, \`e possibile specificare che se la riga non \`e presente il risultato deve essere NULL.

Si pu\`o definire un intervallo fisico, superiore o inferiore. 
\begin{lstlisting}[language=sql]
    ROWS BETWEEN 1 PRECEDING AND 1 FOLLOWING
\end{lstlisting}
\`E possibile definire la tupla currente e quella che la precedono e che la seguono
\begin{lstlisting}[language=sql]
ROWS UNBOUNDED PRECEDING (o FOLLOWING)
\end{lstlisting}
Il rggruppamento fisico \`e specifico per quando i dati non hanno delle interruzioni.


Per definire un intervallo logico si utilizza il costrutto \textbf{range}.

\begin{lstlisting}[language=SQL]
SELECT Citta, Mese, Importo,
    Importo / SUM(Importo) OVER () AS PerOverMax,
    Importo / SUM(Importo) OVER (PARTITION BY Citta) AS PerOverCity,
    Importo / SUM(Importo) OVER (PARTITION BY Mese) AS PerOverMonth
FROM Vendite
\end{lstlisting}


Se una \textbf{group by} \`e presente all'interno della query allora, tutte le entry che possono comparire nella finestra di calcolo sono solo quelle che compaiono nella group by.

\paragraph{Funzione di ranking}
La funzione di ranking serve a creare delle classifiche

\begin{itemize}
    \item \textbf{rank()}: la funzione rank in presenza di pi\`u oggetti nella stessa posizione salta al prossimo record;
    \item \textbf{denserank()}: la funzione denserank tiene tutte righe con la stessa posizione;
\end{itemize}

...
\begin{lstlisting}[language=sql]
SELECT Citta, Mese, SUM(Importo) AS TotMese,
    RANK() OVER (PARTITION BY Citta
                ORDER BY SUM(Import) DESC)

FROM Vendite, ...
WHERE ...
GROUP BY Citta, Mese
\end{lstlisting}



\paragraph{Estensione della group by}
\begin{itemize}
    \item \textbf{rollup}: consente di calcolare le aggragazioni su tutti i possibili gruppi, eliminando una colonna alla volta, da destra verso sinistra, esempio: calcola le vendite per: (Citta, Mese, Prodotto), (Citta, Mese), (Citta):
        \begin{lstlisting}[language=sql]
SELECT Citta, Mese, Prodotto, SUM(Importo) AS TotVendite
FROM ...
WHERE ...
GROUP BY ROLLUP (Citta, Mese, Prodotto)
        \end{lstlisting}
    \item \textbf{cube}: consente di calcolare tutte le possibili combinazioni del ragrruppamento;
    \item \textbf{grouping sets}: serve a definire degli aggregati su gruppi specifici definiti dall'utente;
\end{itemize}


\subsection{Sintassi ORACLE}
Raggruppamento fisico:
\begin{example}{}{}
    Selezionare, separatamente per ogni citt\`a, per ogni data l'importo e la media dell'importo dei due giorni precedenti.
    \begin{lstlisting}[language=sql]
select citta, data, importo,
    avg(importo) over (partition by citta
                        order by data
                        rows 2 preceding
    ) as mediaMobile
from vendite
order by citta, data;
    \end{lstlisting}
\end{example}


Raggruppamento logico:
\begin{example}{}{}
   \begin{lstlisting}[language=sql]
select citta, data, importo,
    avg(importo) over (PARTITION BY citta
                        ORDER BY data
                        RANGE BETWEEN INTERVAL '2'
                        DAY PRECEDING AND CURRENT ROW
    ) as mediaUltimi3Giorni
from vendite
order by citta, data;
   \end{lstlisting}
\end{example}

\begin{example}{}{}
    \begin{lstlisting}[language=sql]
select COD_A, sum(Q) as sommaPerArticolo,
    rank() over (order by sum(Q) desc) as graduatoria
from FAP
group by COD_A
    \end{lstlisting}
\end{example}


All'interno di oracle sono preseti delle funzionalit\`a aggiuntive oltre alla funzione di rank, come: 

\paragraph{ROW\_NUMBER}
Assegno un numero progressivo ad ogni elemento in una partizione.

\begin{lstlisting}[language=sql]
select tipo, peso,
    row_number over (partition by tipo
                    order by tipo)
from ...
where ...;
\end{lstlisting}

\paragraph{CUME\_DIST}
Consente di calcolare le distribuzine cumulativa all'interno di una partizione, permette di definire un valore sulla distribuzione dei valori.


\paragraph{NTILE(n)}
Una funzione che da la possibilit\`a di dividere le partizioni in sottogruppi
\begin{lstlisting}[language=sql]
select tipo, perso, 
    ntile(3) over (partition by tipo order by peso) as ntile3peso
from ...
where ...;
\end{lstlisting}



\subsection{Esercizi}
Cliente(CodCliente, Cliente, Provincia, Regione) \\
Categoria(CodCat, Categoria) \\
Agente(CodAgente, Agente, Agenzia) \\
Tempo(CodTempo, Mese, Trimestre, Semestre, Anno) \\
Fatturato(CodTempo, CodCliente, CodCatArticolo, CodAgente, TotFatturato, NumArticoli, TotSconto) 

\vspace{.8cm}

1. Visualizzare per ogni categoria di articoli
\begin{itemize}
    \item la categoria
    \item la quantità totale fatturata per la categoria in esame
    \item il fatturato totale associato alla categoria in esame
    \item il rank della categoria in funzione della quantità totale fatturata
    \item il rank della categoria in funzione del fatturato totale
\end{itemize}
\begin{lstlisting}[language=sql]
select categoria, sum(numArticoli),
    sum(totFatturato),
    rank() over (order by sum(numArticoli) desc),
    rank() over (order by sum(totFatturato) desc)
from fatturato f, categoria c
where f.codCatArticolo = c.codCat
group by categoria;
\end{lstlisting}
2. Visualizzare per ogni provincia
\begin{itemize}
    \item la provincia
    \item la regione della provincia
    \item il fatturato totale associato alla provincia
    \item il rank della provincia in funzione del fatturato totale, separato per regione
\end{itemize}
\begin{lstlisting}[language=sql]
select provincia, regione,
    sum(totFatturato) as fatturatoPerProvincia,
    rank() over (partion by regione
                 order by sum(totFatturato) desc
    ) as rankFatturatoPerRegione
from cliente c, fatturato f
where c.codCliente = f.codCliente
group by provincia, regione;
\end{lstlisting}
3. Visualizzare per ogni provincia e mese
\begin{itemize}
    \item la provincia
    \item la regione della provincia
    \item il mese
    \item il fatturato totale associato alla provincia nel mese in esame
    \item il rank della provincia in funzione del fatturato totale, separato per mese
\end{itemize}
\begin{lstlisting}[language=sql]
select provincia, regione, mese,
    sum(totFatturato) as fatturatoPerProvinciaPerMese,
    rank() over (partition by mese
                order by sum(totFatturato)
    ) as rankFatturatoPerMese
from cliente c, fatturato f, tempo t
where c.codCliente = f.codCliente and t.codTempo = f.codTempo
group by provincia, regione, mese;
\end{lstlisting}
4. Visualizzare per ogni regione e mese
\begin{itemize}
    \item la regione
    \item il mese
    \item il fatturato totale associato alla regione nel mese in esame
    \item l’incasso cumulativo al trascorrere dei mesi, separato per ogni regione
    \item l’incasso cumulativo al trascorrere dei mesi, separato per ogni anno e regione
\end{itemize}
\begin{lstlisting}[language=sql]
select regione, mese,
    sum(TotFatturato) as fatturatoPerMese,
    sum(TotFatturato) over (
        partition by regione
        order by mese
        rows unbounde preceding
    ) as incassoCumulativoTot,
    sum(TotFatturato) over (
        partition by regione, anno
        order by mese
        rows unbounded preceding
    ) as incassoCumulativoPerAnno
from cliente c, fatturato f, tempo t
where c.CodCliente = f.CodCliente and t.CodTempo = f.CodTempo
group by regione, mese, anno;
\end{lstlisting}



\newpage
\section{Viste materializzate}
Le viste materializzate sono necessarie per ridurre la lentezza della operazioni di group by per grandi moli di dati, le viste materializzate sono dei sommari precalcolati della tabella dei fatti.

Le VM usano con con costruto pricipale la group by, quando si crea una VM \`e convienente includere anche le dimensioni a granularit\`a superiori, in modo da poter riutilizzare la tabella.

Per rappresetnare le dipendenze delle viste materializzate si utilizza un \textbf{reticolo multidimensionale}. Pi\`u ci si trova in alto al reticolo pi\`u ci si avvina alle dimensioni della tabella dei fatti, pi\`u si va in basso pi\`u si trova un granuralit\`a maggiore.

La scelta delle viste viste tra tutte le possibili combinazioni \`e data da:
\begin{itemize}
    \item si scelgie una sola vista da cui \`e possibile raggiungere tutti gli attributi;
    \item creo una vista per ogni query;
    \item scelgo delle viste intermedie che possono portare a ripsondere e pi\`u query;
\end{itemize}

\subsection{Documentazione Oracle}
Riducono i tempi di esecuzione delle group by e non si eseguono pi\`u le join. Nel DBMS Oracle esiste la \textbf{query rewriting}, che permette grazie all'ottimizzatore di interpretare le query e se i risultati corrispondono alle condizioni di creazioni delle viste, allora le query viene riscritta con la vista.
\begin{lstlisting}[language=sql]
create materialized view NAME
[build {immediate|deferred}]
[refresh {complete|fast|force|never}
         {on commit|on demand}]
[enable query rewrite]
as
    QUERY
\end{lstlisting}
\begin{itemize}
    \item immediate: lo schema della tabella viene popolata immediatamente, dato dallo schema di attributi presenti nella select;
    \item deferred: la vista viene creata, ma viene popolata successivamente;
    \item complete: i dati vengono presi interamente dal database;
    \item fast: i dati vengono presi in modo incrementale;
    \item force: se possibile viene eseguito il refresh in modalit\`a fast, oppure in modalit\`a complete;
    \item never: la vista non viene mai aggiornata;
    \item on commit: ogni volta che viene fatto un commit sulla tabella della query anche la vista viene aggiornata;
    \item on demand: viene definito dall'utente quando aggiornare la vista;
    \item enable query rewrite: abilita il dbms ad usare la vista per accellerary le query;
\end{itemize}

Per effetturare il refresh esistono dei tipi di job (a differenza del tipo di prodotto). Quando abbiamo bisogno del fast refresh, la tabella ha bisogno delle informazioni aggiuntive, ovvero dei file di log che ci informano delle nuove informazioni aggiunte al db, la \textbf{materialized view log} \`e associata ad una tabella che ha subito delle variazioni:
\begin{lstlisting}[language=sql]
create materialized view log on
    TABELLA
with sequence, rowid
    (Attributo, ...)
including new values;
\end{lstlisting}
\begin{itemize}
    \item squence: istante temporale in cui \`e avvenuta la modifica;
    \item rowid: indica la tupla che ha subito una modifiche;
\end{itemize}
Su queste keyword si deve difinire una lista di attributi da monitorare, si aggiunge \textbf{including new values} per supportare l'inserimento di nuove tuple.



\newpage
\section{Progettazione fisica}
Fare una progettazione fisica comporta analizzare il carico di
Si difiniscono delle strutture fisiche accessorie per velocizzare le operazioni. Si possono definire delle viste oppure degli indici, ad esempio: indici bitmap, indice di join, ...

La progettazione fisica \`e dipendente dal carico di lavoro. 

La progettazione fisica \`e caratterizzata da una fase di tuning, utilizzata per testare gli indici e le viste create e decidere se matenerli o meno. 

Gli inidici si possono creare sugli attributi che vengono selezionati pi\`u frequentemente, se il dominio \`e ridotto (come quelle categorici dei DW) si utilizza un a bitmap, altrimenti un B-tree.


\newpage
\section{Alimentazione dei Data Warehouse}
Essendo dei dati derivati, la prima operazione da effettuare \`e l'ETL, se questo \`e complesso si va a definire un area di staging in cui il dato viene matenuto temporaneamente. Il processo di ETL va gestito sei per il popolamento del DW sia per quando verr\`a aggiornato con dati nuovi.

\subsection{Estrazione}
L'estrazione statica \`e la prima estrazione effettuata per popolare il DW. Per fare l'estrazione incrementale si possono:
\begin{itemize}
    \item creare delle applicazioni ad hoc per i sistemi legacy;
    \item usando dei log, che non vanno ad interferire con il carico del db;
    \item usando dei trigger: sono proceddure che si attivano quando degli si effettuano delle operazioni specifiche;
    \item basata su timestamp: dove i recordi hanno il timestamp dell'ultima modifica effettuata su di essi;
\end{itemize}

\subsection{Pulitura}
Quando si effettua una estrazione ci si potrebbe trovare di fronte a:
\begin{itemize}
    \item dati duplicati;
    \item dati mancanti;
    \item campo non previsto;
    \item valori errati o impossibili;
    \item inconsistenza del valore;
\end{itemize}
Ogni errore richede una tecnica specifica per essere risolto, le pi\`u comuni sono l'uso di dizionari per controllare errori di battitura, oppure il \textbf{join approssimato}, ad esempio: due database non hanno una chiave condivisa per identificare un utente dall'ordine effettutato, allora per fare una join si dovranno prendere i campi comuni, controllandone sempre la consistenza, oppure i problemi di \textbf{merge/purge}, ad esmpio: facendo il merge di due db le informazioni potrebbero essere duplicate ...

\subsection{Trasformazione}
Conversione dei dati nel formato di quelli presenti nel data warehouse.

\subsection{Caricamento}
In fase di caricamento i dati si caricano nel seguente ordine:
\begin{itemize}
    \item dimensioni;
    \item fatti;
    \item indici e viste;
\end{itemize}




\begin{problem}{Progettazione Magazzini}{progettazione-magazzini}
    \hbadness=99999
    Tabelle: \\
    Tempo(\underline{codT}, data, mese, 3m, 4m, 6m, anno) \\
    Magazzino(\underline{codMa}, magazzino, citta, provincia, regione) \\
    Modello(\underline{codMo}, modello, categoria) \\
    \texttt{UsoMtqMagazzino(\underline{codMa}, \underline{codT}, mtqLiberi, mtqTot) \\
    UsoProdMagazzino(\underline{codMa}, \underline{codMo}, \underline{codT}, numeroProdottiTotale, valoreTotaleProdotii) }

    Query:
    \begin{enumerate}
        \item Relativamente al primo trimestre dell’anno 2013, considerando solo i magazzini della città di Torino,trovare per ogni coppia (magazzino,data) il valore complessivo di prodotti presenti in tale data nelmagazzino e il valore complessivo medio giornaliero di prodotti presenti nel magazzino nel corsodella settimana precedente la data in esame (data in esame inclusa):
\begin{lstlisting}[language=sql]
select magazzino, data,
    sum(valoreTotProdotti) as valoreTot,
    avg(sum(valoreTotProdotti)) over (
        partition by magazzino
        order by data
        range between interval '7'
        day preceding and current row
    ) as valoreMedioSuGiornoCorrenteESettimanaPrecedente

from UsoProdMagazzino u, Tempo t, Magazzino m
where u.codT = t.codT and
    u.codMa = m.codMa and
    citta = 'torino' and
    anno = 2013 and
    3m = 1
group by magazzino, data;
\end{lstlisting}
        \item Relativamente all’anno 2004, trovare per ogni coppia(città,data) la percentuale di superficie liberagiornaliera nella città. Associare ad ogni coppia un attributo di rank legato alla percentuale disuperficie libera giornaliera nella città (1 per la coppia con la più bassa percentuale di superficielibera giornaliera).
\begin{lstlisting}[language=sql]
select citta, data,
    sum(mtqLiberi) / sum(mtqTot) * 100 as percentualeMtqLiberi,
    rank() over (
        order by sum(mtqLiberi) / sum(mtqTot) * 100
    ) as rankLowestPercentuale
from Tempo t, Magazzino m, UsoMtqMagazzino u
where t.codT = u.codT and
    m.codMa = u.codMa and
    anno = 2004
group by citta, data;
\end{lstlisting}
        \item Relativamente ai primi sei mesi dell’anno 2014, trovare per ogni coppia (magazzino,data) la percentuale di superficie libera giornaliera.
\begin{lstlisting}[language=sql]
select magazzino, data,
    100 * sum(mtqLiberi) / sum(mtqTot) as percentualeMtqLiberi
from Tempo t, Magazzino m, UsoMtqMagazzino u
where t.codT = u.codT and
    m.codMa = u.codMa and
    anno = 2014 and
    mese <= 6
group by magazzino, data;
\end{lstlisting}
        \item Relativamente all’anno 2013, trovare per ogni coppia (magazzino,mese) il valore complessivo medio giornaliero di prodotti presenti.
\begin{lstlisting}[language=sql]
select distinct magazzino, mese,
    avg(sum(valoreTotProdotti)) over (
        parition by magazzino, mese
    ) as valoreMedioGiornalieroComplessivo
from UsoProdMagazzino u, Tempo t, Magazzino m
where u.codMa = m.codMa and
    u.codT = t.codT and
    anno = 2013
group by magazzino, mese, data;
\end{lstlisting}
        \item Relativamente all’anno 2015, trovare per ogni regione il valore complessivo medio giornaliero di prodotti presenti nella regione.
\begin{lstlisting}[language=sql]
select regione, mese,
    avg(sum(valoreTotProdotti)) over (
        parition by regione, mese
    ) as valoreMedioGiornalieroComplessivo
from UsoProdMagazzino u, Tempo t, Magazzino m
where u.codMa = m.codMa and
    u.codT = t.codT and
    anno = 2015
group by regione, mese, data;
\end{lstlisting}
        \item Relativamente all’anno 2014, trovare per ogni coppia(mese, regione) la percentuale di superficie libera giornaliera nella regione.
\begin{lstlisting}[language=sql]
select regione, mese,
    avg (100 * sum(mtqLiberi) / sum(mtqTot)) over (
        partition by regione
    )
from UsoMtqMagazzino u, Tempo t, Magazino m
where u.codMa = m.codMa and
    u.codT = t.codT and
    anno = 2014
group by regione, mese, data;
\end{lstlisting}
    \end{enumerate}
\end{problem}




\section{Data Lake}
I data lake sono dei repositori di dati, storicizzati per utilizzo futuro cos\`i come sono disponibili, in qualsiasi formato. Questi raw data potrebbero essere utilizzati in futuro.

I data lake danno la possibilit\`a di storicizzare i dati per un uso futuro, inoltre tutti i dati sono contunuti in un repository comune, infatti \`e caratterizzato da bassi costi di storage e mantenimento, per\`o pu\`o essere difficile estrapolare dei dati.

\section{Data processing}
Una collezione \`e costituita da oggetti di dato,
\begin{itemize}
    \item \textbf{Attributo}: \`e una propriet\`a dell'oggetto;
    \item \textbf{Tipi di Attributo}: possono essere nominali, ordinali, intervalli, rapporti;
    \item \textbf{Proprit\`a dei valori degli attributi}: possiamo definire equivalenza, ordine, addizione, moltiplicazione;
    \item \textbf{Attributi discreti e continui}:  discreti hanno un nemero finito di valori, i continui hanno dei valori reali;
\end{itemize}

Tipi di dato da analizzare:
\begin{itemize}
    \item record: sono i dati presenti in una tabella;
    \item grafi: come la struttura di una pagina web;
    \item ordinato: dati in cui esiste il concetto di squenza;
\end{itemize}

Esistono vari tipi di dato:
\begin{itemize}
    \item \textbf{Document Data} Per ogni riga ho un doumento, per ogni colonna ho degli attributi che descrivono delle parole chivi all'interno del documento, ogni riga \`e un array che contiene la pesatura degli attributi, la pesatura pu\`o essre calcolata con algoritmi specifici;
    \item \textbf{Dato transazionale} Un dato transazionale \`e formato da un insieme di items all'interno della tabella, ogni transazione \`e identificata da un ID, nelle tabelle transazionali non esiste il concetto di ordine ne tra le riche, ne all'interno della transazione;
    \item \textbf{Dato a grafo}: ad esempio le pagine web hanno dei link ad altre pagine, potrei considere i link come gli archi del grafo, ognuno con un peso specifico, e la singola pagina come un nodo del grafo;
    \item \textbf{Qualit\`a del dato}: nella maggior parte dei casi i dati presentano delgi errori, questi possono essere causati da rumori e outliers, dati mancanti o duplica, ...;
    \item \textbf{Outliers}: dati che escono al di fuori del comportamento medio e quindi molto rumorosi, l'abbiettivo dell'analisi di outlier detection \`e l'individuazione di questi dati e dell'eliminazione;
\end{itemize}




\subsection{Data aggregation}
Consente di combinare pi\`u record o pi\`u attributi, si fa questo per effettuare una riduzione della quantit\`a di dati, ed avere dei dati pi\`u stabili con una vairiabilit\`a minore.

\subsection{Data reduction}
Si possono effetuare due tipi di riduzione: riduzione degli attributi o riduzione dei valori. Per effettuare queste riduzioni esistono delle tecniche specifiche.

Una di esse \`e il \textbf{sampling} \`e una tecnica di statistica di analisi, ad esempio nella pipeline di datascience il sampling serve per trovare delle rappresentazioni adatte al dataset, facendo N esperimenti su un sample preso dal dataset si andranno a fare dei test sull'intero dataset per verificare che le ipotesi siano confermate, in figura si pu\`o vedere come il grafico col numero pi\`u piccolo di campia non sia pi\`u rappresentativo del dataset originale.
\begin{figure}[H]
    \centering
    \includegraphics[width=0.8\textwidth]{sampling-example.png}
    \caption{Sampling Example}
    \label{fig:sampling-example}
\end{figure}
I tipi di sampling possono essere:
\begin{itemize}
    \item randomici: un dato estratto non viene reinserito;
    \item con rimpiazzo: un dato estratto pu\`o essere riestratto;
    \item sampling stratificato (pi\`u utilizzato): si pu\`o stratificare il dataset a partire da uno o pi\`u attributi (le partizioni vengono detti bucket), e poi vengono presi dei valori casuali;
\end{itemize} 
Il problema con questi titi di approccio \`e che all'umentare delle dimensioni i dati diventano pi\`u distanti tra loro. Per ridurre gli effetti di avere troppe dimnesioni si possono applicare delle tecniche di riduzione dimensionale. Per questo motivo vengono utilizzate tecniche di \textbf{dimensionality reduction}, alcune di queste sono:
\begin{itemize}
    \item \textbf{pca, svd, ...}: tecniche statistiche;
    \item \textbf{feature selection}: si vuole avere la proiezione del dato su un nuovo attributo in modo da aumentare la varianza, in qualche modo ridurre le feature ridondanti: le tecniche di feature selection sono: brute force; embedded approach (tramite un albero di selezione si selezionano solo i dati pi\`u significativi, pu\`o essere fatto quando gli attributi non sono elevati); filter (basiti sull'analisi di correlazione, per verificare se esistono delle correlazioni lineari); wrapper (viene utilizzato del data mining come black-box per identificare delle combinazioni di feature); feature creation (consiste nel combinare pi\`u variabili in una solo variabile, viene effettuato anche un processo di feature selection);
    \item \textbf{discretizzazione}: per effettuare una discretizzazione si deve mappare un valore continuo in un range di numeri discreti, una tecnica \`e quella di generare degli intervalli di uguale lunghezze e se la variabile ricade un in intrvalallo gli viene associato il simbolo corrispondente, questo potrebbe modellare dei volori outliers o rumorosi, si potrebbe anche usare del clustering, ovvero l'aggregazione di dati in base alla distanza tra vari valori, solitamente vengono usate due tecniche per validare i dati dopo la pipeline. Un caso particolare della discretizzazione \`e la \textbf{binarizzazione} che consiste nel discretizzare una variabile e poi  viene fatto il one-hot encoding (i valiri vengono mappati su una bitmap);
    \item \textbf{trasformazione}: un attributo va trasformato quando si vogliono riportare i valori in un'altra scala, una delle tecniche pi\`u comuni \`e la normalizzazine, ad esempio negli algoritmi di clustering viene definito uno spazio normato per calcolare la distanza tra i valori, le normalizzazioni pi\`u usate sono:
        \begin{theorem}{min-max}{min-max}
            \[ v' = \frac{v - min}{max - min} (new\_max - new\_min) + new\_min \]
        \end{theorem}
        \begin{theorem}{z-score}{z-score}
            \[ v' = \frac{v - mean}{stand\_dev} \]
        \end{theorem}
        \begin{theorem}{decimal scaling}{decimal-scaling}
            \[ v' = \frac{v}{10^{j}} \]
            $j$ intero più piccolo tale che: $\max (|v'|) < 1$
        \end{theorem}
\end{itemize}

La similarit\`a e la dissimilarit\`a ci permattono di dire quando degli attributi sono simili o dissimili tra di loro, la similarit\`a viene espresso in un intrevallo $[0, 1]$, con 1 = identici, per definire la similarit\`a si definisce un concetto di distanza, ed una \textbf{matrice di similarit\`a}, in cui ogni riga e colonna sono presenti i valori, ogni celle rappresenta le distanze tra i due valori. Le tre distanze utlizzate sono: Manhattan, Euclidea, Mikowski. In coso queste distanze non soddifisfino i criteri si definisc una distanza attraverso uno spazio vettoriale.

Una distanza importante \`e la \textbf{distanza di mahalanobis}, che mostra quanto due punti sono distanti in una distribuzione.
\begin{theorem}{Mahalanobis}{mahalanobis}
    \[ Maha(\vv{x}, \vv{y}) = (\vv{x} - \vv{y})^{T} \Sigma^{-1} (\vv{x} - \vv{y}) \]
    $\Sigma$ matrice delle covarianze.
\end{theorem}

La correlazione \`e molto importante per trovare relazioni tra i dati. Una tecnica molto usata \`e trovare le combinazioni lineari attraverso il coefficente di pearson.



\newpage
\section{Regole di associazione}
L'estrazione delle regole di associazione \`e un modo di trovare delle associazioni tra i volori prenti in un database transazionale. Per decidere queste associazioni solitamente si guardano le ricorrenze statistiche di valori comuni.

Una regola di associazione si definisce come:
\begin{theorem}{Regola di associazione}{regola-di-associazione}
    \[ A, B \implies C \]
    Dove degli insiemi di oggetti (\textbf{itemset}) possono implicare altri insiemi.
    \begin{itemize}
        \item $A, B =$ corpo della regola;
        \item $C =$ testa della regola;
    \end{itemize}
    La freccia indica la \textbf{co-occorrenza}, indica che il copro \`e legato alla testa nelle transazioni del db. Per esempio: coca, pannolini $ \implies $ latte.
\end{theorem}

Se si lavora con un db relazione possiamo estrarre una transazione associando ad ogni valore il suo attributo.

Definizioni:
\begin{definition}{k-itemset}{k-itemset}
    \`E un itemset che contiene k oggetti.
\end{definition}
\begin{definition}{support count}{support-count}
    \`E la frequenza con cui una trasazione appare.
    \[ \#{Roma, Colosseo} = 2 \]
    \[ sup(Roma, Colosseo) = 2 \]
\end{definition}

Data una regola di associazione $A \implies B$:
\begin{definition}{Supporto}{supporto}
    \[ \frac{\#{A,B}}{|T|} \]
    \`E la frazione di transazioni che contengono sia A e B. $|T|$ \`e la cardinalit\`a del db.
\end{definition}
\begin{definition}{Confidenza}{confidenza}
    \[ \frac{sup(A,B)}{sup(A)} \]
    \`E la frequenza delle transazioni B che contengono anche A.
\end{definition}

Per creare dei modelli per estrarre  le relazioni, vengono definiti due paramtri che indicano la frequenza con quele frequnza devono apparire le relazione:
\begin{itemize}
    \item supporto $>$ \textbf{minsup} threshold;
    \item confidenza $>$ \textbf{minconf} threshold;
\end{itemize}
Questo viene fatto  per limitare il numero di relazioni che vengono estratte, perch\'e nella maggior parte dei casi i db sono molto grandi.

L'estrazione si compone di due fasi:
\begin{enumerate}
    \item si estraggono gli itemset frequenti, attraverso il vincolo sul supporto;
    \item si estraggono le regole di associazione, attraverso il vincolo sulla confidenza;
\end{enumerate}
Il passo pi\`u oneroso \`e l'estrazione degli itemset frequenti.

Il \textbf{candidato} \`e l'oggetto che potrebbe essere estratto, il \textbf{frequente} \`e l'oggetto che supera il valore di minsup. Per migliorare 

\subsection{Algoritmi Di Estrazione Di Itemsets}
\subsubsection{Apriori}
\textbf{Apriori} si basa sul principio di quanto un itemset \`e frequnte, allora la porzione di lattice che ha come radice l'itemset allora si pu\`o esplorare il sottospazione dell'itemset, altrimenti no.
\[ A \subset B \implies sup(A) \geqslant  sup(B) \]

L'algoritmo di \textbf{Apriori} si basa sulla suddivide degli'itemset in  livelli, in ogni livello si prendono dei candidati, facendo il join tra candidati frequenti (che superano la minconf) di livello k si generano candidati di livello k+1, per itemset che non rispettano il criterio di frequenza veiene fatto il pruning dell'albero delle scelte.

Sui candidati di lunghezza 2 va applicato il pruning (apriori). Alla fine si trover\`a l'insieme delle soluzioni che sar\`a l'unione di tutti gli itemset estratti.

\tolerance=1000
Le limitazioni principali di questo algoritmo sono i costi di scansione del database, infatti dovr\`a essere letta pi\`u volte, inoltre se le transazioni sono molto lunghe l'algoritmo dovr\`a essere ripetuto n+1 volte (n = lunghezza transazoine), per superare questo limite si posso usare degli algoritmi per diminuire i costi legati alla lettura.


\subsubsection{FP-Growth}
Sono state proposte delle varianti dell'algoritmo per ottimizzare i problemi . Negli anni 2000 \`e stato proposto un nuovo algoritmo basato sulla memorizzazione. L'algoritmo \textbf{FP-growth}, instazia in memoria un albero dove si trova la proiezione del database originale che considera gli item che soddisfano la soglia di supporto, una volta creata la struttura di supporto in memoria non si accede pi\`u in memoria secondaria, ottimizzando la lettura degli item. La struttura in memoria prende il nome di \textbf{FP-tree}. L'algoritmo funziona nel seguente modo:
\begin{enumerate}
    \tolerance=1000
    \item vengono contati gli item e vengono scartati quelli con sotto la soglia;
    \item viene creata una header table con gli item in orine descrescente rispetto al supporto;
    \item viene scansionata per l'ultima volta il database, le transazioni vengono ordinate in base alla header table;
    \item ogni scansione di una transazine ordinata, viene preso l'item ed inserito nell'albero, ogni nodo contiene l'item ed il numero di volte che \`e stato trovato per ogni transazione letta;
    \item l'inserimento nell'albero viene fatto a partire dall'ordine degli item nella transazione (simile algi alberi formati a partire da ogni lettera di di parole), ogni volta che si passa da un nodo gi\`a inserito il suo contatore aumenta;
    \item \`e importante collegare la header table ai nodi nell'albero, questo viene fatto attraverso una \textbf{node link chain}: l'header punta ad un nodo con lo stesso item, quando percorrendo l'abero si trova un altro item il nodo precedente avr\`a come nodo successivo il nodo corrente;
\end{enumerate}

Viene poposto anche un algoritmo di visita per esrarre gli itemset:
\begin{enumerate}
    \item viene letta la header table dall'item col supporto pi\`u basso;
    \item viene creato un \textbf{conditional pattern base} di un item, una proiezione dell'fp-tree condizionato ad un item;
    \item il CPB viene visitato ricorsivamente per travore i candidati;
    \item ad ogni nodo visitato viene recuperato il path dell'albero fino ad esso, se il supporto del nodo \`e minore del minsup il path viene scartato, e si passo al prossimo nodo della node-link chain;
        \begin{figure}[H]
            \centering
            \includegraphics[width=0.8\textwidth]{cpd-di-d.png}
            \caption{Cpd di D al terzo step}
            \label{fig:cpd-di-d}
        \end{figure}
    \item data la cpb va creata una Conditional header table, da questa header table, si crea un'altro fp-tree, questo si applica ricorsivamente, ogni chiamata \`e condizionata (ovvero la cpd sar\`a preceduta dall'itemset del chiamante: D -> DC -> ...);
    \item quando non si riesce a creare una header table si torna al chiamante e si passa alla entry superiore nella header table del chiamante;
    \item prima di creare la nuova cpb, si prende l'itemset che arriva dal chiamante e se il valore nell'header tabel del item corrente \`e maggiore del minsup allora l'itemset concatenato all'item corrente viene insireti negli itemset frequenti;
        \begin{figure}[H]
            \centering
            \includegraphics[width=0.8\textwidth]{esempio-aggiunta-itemset-nei-valori-frequenti.png}
            \caption{Esempio Aggiunta Itemset Nei Valori Frequenti}
            \label{fig:esempio-aggiunta-itemset-nei-valori-frequenti}
        \end{figure}
\end{enumerate}
Questo algoritmo funziona molto bene se la memoria non viene saturata (bottleneck).

Un altro problema sono l'alto numbero di itemset che vengono estratti, infatti anche con db molto piccoli si possono trovare un numero molto elevato di iteset frequenti, per questo si opta per i \textbf{itemset massimali frequenti}, IMF per definizioni sono gli itemset che hanno il massimo numbero di figli, e sono unici nel loro sottolivello, oppure esistono i \textbf{closed itemset} che sono gli itemset con nessuno dei suoi immediati superset hanno lo stesso valore.

\subsection{Effetto delle soglie}
La scelta dei valori di supporto deve essere idoneo, infatti con un valore troppo basso non si identificano delle relazioni e con valori troppo alti emergono relazioni molto deboli. Anche la confidenza comporta dei problemi, se il valore di cui si calcola la confidenza ha un valore molto ampio, si rischiano di ottenere valori errati, per evitare di incrociare queste informazioni si utilizza la regola del lift.
\begin{definition}{Lift}{lift}
    Dato $ r: A \implies B $, allora la Correlazione o lift \`e:
    \[ C = \frac{P(A,B)}{P(A)P(B)} = \frac{conf(r)}{sup(B)} \]
\end{definition}


Un esempio di regola di associazione potrebbe anche essere l'aggregazione di dati, andano ad accoppirare una tassonomia ai valori, possiamo, aggregano gli attribuiti, vedere il loro supporto crescere, rappresetando in modo generalizzato un comportamento, per andare a soddifare un servizio.


\newpage
\section{Classificazione}
Le classificazioni cercano, attraverso dei modelli di assegnare dei tag ai dati, attraverso delle tecniche supervised (vuol dire che abbiamo gi\`a a disposizione un pool di dati da cui possiamo estrapolare le informazioni per assegnare un tipo di tag).

Per applicare la classificazione si ha bisogno di dei dati di training che hanno gia dei tag con il quale si va a generare un modello, per classificare dei nuovi dati si parte dandoli in pasto al modello e partendo dai volori degli attributi si generano delle nuove etichette.

Per poter realizzare un modello di classificazione si ha bisogno di dati di training, usati per generare il modello, e dati di test, usati per validare il modello, ognuno di questi dati ha gi\`a associato ad essi una classe di tag. Una volta che si trova un modello adatto, si pu\`o insireri in un applicazione per predirre i tag.

Gli algoritmi che generano i modelli hanno delle caratteristiche:
\begin{itemize}
    \item accuratezza;
    \item interpretabilit\`a;
    \item incrementalit\`a: il modello pu\`o essere aggiornato all'arrivo di nuovi dati;
    \item efficenza;
    \item scalabilit\`a: performance dell'algoritmo rispetto al numero di dati;
    \item robustezza: capcit\`a dell'algoritmo di operare in presenza di dati rumorosi o mancanti;
\end{itemize}

\subsection{Alberi di Decisione}
Attraverso un albero di decisione, dati i dati di input \`e possibile inferire la classe di etichetta.
\begin{figure}[H]
    \centering
    \includegraphics[width=0.8\textwidth]{albero-di-descisione.png}
    \caption{Albero Di Descisione}
    \label{fig:albero-di-descisione}
\end{figure}
In un albero di decisione le folgie corrispondo all'etichetta di classe che sat\`a assegnata alla entry. Questo modello dipendo molto dai dati, infatti se gli attributi cambiano anche il modello va modificato opportunamente. Per generare gli alberi di decisione esistono degli algoritmi, un di questi \`e l'\textbf{algoritmo di Hunt}. L'algoritmo parte dal leggere il database, e si cerca di  individuare l'attributo che ha la maggiore capacit\`a di dividere il db in due sottogruppi, il primo passo \`e duqnue individuare l'attributo che riesce a dividerere il db in due gruppi pi\`u omogenei possibili rispetto al tag. Successivamente si cerca il prossimo attributo che effettua il prossimo miglior partizionamento. Questo albero non \`e aggiornabile all'arrivo di nuovi dati.

Per effettuare lo split bisogna partire dai tipi di dati con cui si lavora:
\begin{itemize}
    \item attributo categorico: si possono effettuare n-split diversi per ogni valore, se il tag \`e binario allora i valori vengono partizionati in due gruppi;
    \item attributo numerici: si pu\`o usare la discretizzazione, oppure si pu\`o usare una condizione di test;
\end{itemize}

Per stimare la purezza (grado di omogeneit\`a) dei nodi che vengono generati, esistono delle metriche per calcolare l'impurit\`a: gini index, entropia, missclassification index. Il primo caso \`e quello di utilizzo del \textbf{Gini index}, con questo metodo viene misurata l'impuit\`a prima e dopo lo split, il gini index si misura con:
\[ GINI(t) = 1 - \sum_{j}^{} (p(j|t))^{2} \]
$p(j|t) = $ frequenza della classe j al nodo t. Pi\`u il gini index si avvicina allo zero, pi\`u la classe \`e impura, pi\`u il valori si avvicina a $(1 - \frac{1}{\text{numro di classi}} )$ pi\`u il nodo \`e impuro, viene preso l'attributo che genera il gini index col valori pi\`u basso. Nelle implementazioni reali vengono fatti dei test utilizzando metriche diverse con successive validazioni.

Per la creazione di un albero va definito anche un criterio di stop:
\begin{itemize}
    \item ...
    \item pre-pruning: se un nodo \`e quasi puro rimuovo non scendno pi\`u nell'albero;
    \item post-pruning: generato l'albero completo vado a tagliare i rami dell'albero con delle caratterisiche troppo specifiche;
\end{itemize}





\newpage
\section{Trigger}
Consideriamo un esmpio di tema di esame.

a:
\begin{lstlisting}[language=sql]
misure: sum(incasso), sum(#consulenze)
tabelle: INCASSO, TEMPO, SERVIZION, SEDE-CONSULENTI
gb: semestre, tipologia-servizio
selezione: regione = 'Lombardia'
\end{lstlisting}

b:
\begin{lstlisting}[language=sql]
misure: sum(incasso), sum(#consulenza)
tabella: INCASSO, SEDE-CONSULENTI, SERVIZIO, TEMPO, AZIENDA
gb: regione, servizio, anno
selezione: Nazionalita = 'Italia' or
    Nazionalita = 'Germania'
\end{lstlisting}

c:
\begin{lstlisting}[language=sql]
misure: sum(incasso), sum(#consulenze)
tabelle: INCASSO, SEDE-CONSULENTI, SERVIZIO, TEMPO
gb: tipologia-servizio, regione, semestre
selezione: anno >= 2017 and anno <= 2019
\end{lstlisting}

Date le query precedenti si crei una vista materializzata:
\begin{lstlisting}[language=sql]
-- Query blocco A
select servizio,
    tipologia-servizio,
    semestre,
    anno,
    regione,
    nazionalita,
    sum(incasso),
    sum(#consulenze)
from incasso i, tempo t, azienda a, servizio s, sede-consulenze sc
where condizioni di join
group by servizio,
    tipologia-servizio,
    semestre,
    anno,
    regione,
    nazinalita
\end{lstlisting}

L'identificare minimale sar\`a: (servizio, semestre, regione, nazionalita)

Punto 2:
...

Punto 3:
\begin{lstlisting}[language=sql]
insert into viewIncassi(servizio,
    tipologia-servizio,
    nazionalista,
    semestre,
    anno,
    regione,
    incassotot,
    numconsulenzetot)
(blocco A)
\end{lstlisting}

Punto 4:
\begin{lstlisting}[language=sql]
create trigger refreshViewIncassi
after insert on incasso
for each row
declare
    varServizio varchar(20);
    varTipologiaServizio varchar(20);
    varSemestre varchar(20);
    varAnno varchar(20);
    varNazionalita varchar(20);
    varRegione varchar(20);
    n int;
begin
-- leggere le tabelle dimensionale x recuperare
-- i valori dell'identificatore della vista materializzata:
-- servizio, nazionalita, semestre, regione
select servizio, tipologia-servizio into varServizio,
    varTipologiaServizio
from servizio
where idServizio = :NEW.idServizio;

select semestre, anno
from tempo
where idTempo = :NEW.idTempo;

select nazionalita into varNazionalita
from azienda
where idCategoriaAzienda = :NEW.idCategoriaAzienda

select regione into varRegione 
from sede-consulenti
where idSede = :NEW.idSede;

-- verifico se esiste una tupla in viewIncassi con
-- associati i valori estratti

select count(*) into n
from viewIncassi
where nazionalita = varNazionalita and
    semestre = varServizio and
    servizio = varServizio and
    regione = varRegione;

if (n > 0) then
    update viewIncassi
    set incassoTot = incassoTot + :NEW.incasso
        numConsulenze += :NEW.#consulenze
    where servio = varServizio and
        nazionalita = varNazionalita and
        semestre = varSemestre and
        regiono = varRegione;
else 
    insert into viewIncassi ( ... , )
    values (varServizio,
        varTipoServizio,
        varNazionalita, 
        varSemestre,
        varAnno,
        varRegione, 
        :NEW.incasso,
        :NEW.#consulenze);
end if;
end;
\end{lstlisting}

Punto 5:
\begin{lstlisting}[language=sql]
create trigger updateViewIncassi
after update of tipologiaServizion on servizio
for each row
declare
    typeServizio varchar(20);
begin


end;
\end{lstlisting}

Punto 6:
\begin{lstlisting}[language=sql]
create materialized view log on incasso
with sequence, row id
(...) 
including new values;

create materialized view log on servizio
with sequence, row id
( ... )
including new values;

create materialized view log on tempo
with sequence, row id
(. ...)
including new values;
\end{lstlisting}




\end{document}
